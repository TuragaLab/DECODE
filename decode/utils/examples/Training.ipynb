{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:23:30.081887Z",
     "start_time": "2021-03-16T17:23:24.297709Z"
    }
   },
   "outputs": [],
   "source": [
    "import decode\n",
    "import decode.utils\n",
    "import decode.neuralfitter.train.live_engine\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"DECODE version: {decode.utils.bookkeeping.decode_state()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECODE - Training\n",
    "\n",
    "This notebook highlights how to generate a training parameter file and\n",
    "train a DECODE model for fitting experimental data.\n",
    "\n",
    "The DECODE model is trained on simulated data. This requires a calibration spline based PSF model,\n",
    "and a parameter file that contains camera settings and other hyper parameters to set up the simulation process.\n",
    "\n",
    "While we provide an example calibration file here to demonstrate the training procedure you will have to create one yourself for your own data.\n",
    "\n",
    "If you plan to use DECODE regularly you should take a look at the generated parameter file at the end of this notebook\n",
    "and will recognise that it just holds a couple of parameters and paths. You will want to adjust these\n",
    "to your data by creating or modifying the .yaml file directly.\n",
    "After that you can start the training directly (see the last section of this notebook).\n",
    "Alternatively, SMAP can create this parameter file for you (please consult the documentation for that).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters\n",
    "Set device for training. We **do not recommend training on CPU** since this will be quite slow. If you train on CPU though, you may want to change the number of threads if you have a big machine (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:23:30.091508Z",
     "start_time": "2021-03-16T17:23:30.086762Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0'  # or 'cpu'\n",
    "device_ix = 0  # possibly change device index (only for cuda)\n",
    "threads = 4  #  number of threads, useful for CPU heavy computation. Change if you know what you are doing.\n",
    "worker = 4  # number of workers for data loading. Change only if you know what you are doing.\n",
    "\n",
    "torch.set_num_threads(threads)  # set num threads\n",
    "\n",
    "if device != 'cpu':\n",
    "    if (not torch.cuda.is_available()) or (not decode.simulation.psf_kernel.CubicSplinePSF.cuda_is_available()):\n",
    "        raise ValueError(\"You have selected a non CPU device, but CUDA is not available.\"\n",
    "                         \"Refer to CPU version or check your installation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you have manually downloaded the example package *(experimental_data_workflow)* you can specify\n",
    "the root directory of the .zip in the path variable below. Otherwise, the package will be\n",
    "loaded/reloaded."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:23:33.315488Z",
     "start_time": "2021-03-16T17:23:30.094675Z"
    }
   },
   "outputs": [],
   "source": [
    "# load example calibration file from server (this can be skipped)\n",
    "gateway = decode.utils.example_helper.load_gateway()\n",
    "\n",
    "# dir where to store example data, leave as '' to store in current folder\n",
    "path = Path('')\n",
    "\n",
    "# change here for other files\n",
    "package = gateway['examples']['experimental_data_workflow']\n",
    "\n",
    "# get paths to files\n",
    "zip_folder = decode.utils.example_helper.load_example_package(\n",
    "    path=(path / package['name']).with_suffix('.zip'), url=package['url'], hash=package['hash'])\n",
    "\n",
    "calib_file = str(zip_folder / 'spline_calibration_3dcal.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bead calibration\n",
    "\n",
    "To obtain the calibration file a spline model is fit on a stack of beads.\n",
    "We recommend to use fit3Dspline which is part of the SMAP package.\n",
    "\n",
    "1. Install the stand-alone version of SMAP from [www.rieslab.de](www.rieslab.de) or if you have Matlab, downlowd the source-code from [www.github.com/jries/SMAP](www.github.com/jries/SMAP). There, you also find the installation instructions and Documentation.\n",
    "2. Acquire z-stacks with fluorescent beads (e.g. 100 nm beads). We typcally use a z-range of +/- 750 nm and a step size of 10-50 nm.\n",
    "3. In SMAP, use the plugin *calibrate3DSplinePSF* to generate the calibartion file. In the user guide (accessible from the SMAP help menu) in section 5.4, this is explained in detail. Further information about the calibration process can be found in [Li et al, Nature Methods (2018)](https://doi.org/10.1038/nmeth.4661)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:23:33.320565Z",
     "start_time": "2021-03-16T17:23:33.317982Z"
    }
   },
   "outputs": [],
   "source": [
    "# uncomment and exectue only if you have not executed the example download cell above\n",
    "# calib_file = 'examples/2020-07-23_Pos0_beads_Z_1_MMStack_Default.ome_3dcal.mat'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the simulation parameters we load the default config file and go through the settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:23:33.337627Z",
     "start_time": "2021-03-16T17:23:33.322405Z"
    }
   },
   "outputs": [],
   "source": [
    "# copy default parameter files which are then changed as specified\n",
    "decode.utils.param_io.copy_reference_param('')  # saved in current dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:23:33.362833Z",
     "start_time": "2021-03-16T17:23:33.340478Z"
    }
   },
   "outputs": [],
   "source": [
    "param = decode.utils.param_io.load_params('param_friendly.yaml')  # change path if you load custom file\n",
    "\n",
    "param.Hardware.device = device\n",
    "param.Hardware.device_ix = device_ix\n",
    "param.Hardware.device_simulation = device\n",
    "param.Hardware.torch_threads = threads\n",
    "param.Hardware.num_worker_train = worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The camera parameters need to be adjusted according to the device used. Here we used an EMCCD camera, for a sCMOS device you must set the em_gain to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:23:33.372404Z",
     "start_time": "2021-03-16T17:23:33.364696Z"
    }
   },
   "outputs": [],
   "source": [
    "param.Camera.baseline = 398.6\n",
    "param.Camera.e_per_adu = 5.0\n",
    "param.Camera.em_gain = 100\n",
    "param.Camera.px_size =[127.0, 117.0] # Pixel Size in nano meter\n",
    "param.Camera.qe = 1.0                # Quantum efficiency\n",
    "param.Camera.read_sigma = 58.8\n",
    "param.Camera.spur_noise = 0.0015\n",
    "\n",
    "param.Camera.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation parameters should be set so that the resulting simulated frames resemble real frames as closely as possible.\n",
    "You can use SMAP to infer these parameters by performing inference with an iterative approach on a couple of frames:\n",
    "\n",
    "1. Use the bead calibration to fit your SMLM data.\n",
    "2. Use the plugin: *DECODE\\_training\\_estimates* to estimate the photo-physical parameters of the experiment and to save them into a parameter file. Consult the information of the plugin (accessible via the Info button) for further information.\n",
    "\n",
    "However it is also possible to find reasonable values by hand which we do now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:23:33.381423Z",
     "start_time": "2021-03-16T17:23:33.375116Z"
    }
   },
   "outputs": [],
   "source": [
    "param.Simulation.bg_uniform = [20.0, 200.0]           # background range to sample from. You can also specify a const. value as 'bg_uniform = 100'\n",
    "param.Simulation.emitter_av = 25                      # Average number of emitters per frame\n",
    "param.Simulation.emitter_extent[2] = [-800, 800]    # Volume in which emitters are sampled. x,y values should not be changed. z-range (in nm) should be adjusted according to the PSF\n",
    "param.Simulation.intensity_mu_sig = [7000.0, 3000.0]  # Average intensity and its standard deviation\n",
    "param.Simulation.lifetime_avg = 1.                     # Average lifetime of each emitter in frames. A value between 1 and 2 works for most experiments\n",
    "\n",
    "param.Simulation.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we provide the path to the calibration file, and to the output destination.\n",
    "There are more parameters (you can just execute param in a cell to look at them) that you should not need to change.\n",
    "You might have to reduce the batch size (param.HyperParameter.batch_size) if you run out of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:23:33.390111Z",
     "start_time": "2021-03-16T17:23:33.383578Z"
    }
   },
   "outputs": [],
   "source": [
    "param.InOut.calibration_file = calib_file\n",
    "param.InOut.experiment_out = ''\n",
    "\n",
    "param.InOut.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up our simulator and the camera model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:23:33.651403Z",
     "start_time": "2021-03-16T17:23:33.391971Z"
    }
   },
   "outputs": [],
   "source": [
    "simulator, sim_test = decode.neuralfitter.train.live_engine.setup_random_simulation(param)\n",
    "camera = decode.simulation.camera.Photon2Camera.parse(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:23:33.653389Z",
     "start_time": "2021-03-16T17:23:24.281Z"
    }
   },
   "outputs": [],
   "source": [
    "# finally we derive some parameters automatically for easy use\n",
    "param = decode.utils.param_io.autoset_scaling(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us sample a set frames, and also load our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:23:33.654122Z",
     "start_time": "2021-03-16T17:23:24.284Z"
    }
   },
   "outputs": [],
   "source": [
    "tar_em, sim_frames, bg_frames = simulator.sample()\n",
    "sim_frames = sim_frames.cpu()\n",
    "\n",
    "frame_path = zip_folder / 'frames.tif'  # change if you load your own data\n",
    "data_frames = decode.utils.frames_io.load_tif(frame_path).cpu()\n",
    "\n",
    "print(f'Data shapes, simulation: {sim_frames.shape}, real data: {data_frames.shape}')\n",
    "print(f'Average value, simulation: {sim_frames.mean().round()}, real data: {data_frames.mean().round()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the mean brightness, we see that there is a large missmatch.\n",
    "The reason is that we simulate the photons that are emitted by the fluorophores, while the tiff file shows the photons that are recorded by the camera (after amplificiation).\n",
    "To enable direct comparison we convert the real data frames into photon numbers.\n",
    "Be aware that camera.forward() and camera.backward() are not each others exact inverse. forward()\n",
    "performs noise sampling, while backward() simply converts camera units back to expected photon\n",
    "numbers by the respective statistical means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:23:33.655038Z",
     "start_time": "2021-03-16T17:23:24.288Z"
    }
   },
   "outputs": [],
   "source": [
    "data_frames = camera.backward(data_frames, device='cpu')\n",
    "print(f'Average value, simulation: {sim_frames.mean().round()}, real data: {data_frames.mean().round()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T20:50:07.906886Z",
     "start_time": "2020-10-05T20:50:07.902719Z"
    }
   },
   "source": [
    "By comparing random frames (chosing a dense region of the real data) we can convince ourselves that the distributions are somewhat similar.\n",
    "If you observe large differences for your dataset, you probably have to adjust param.Simulation.bg_uniform and param.Simulation.intensity_mu_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-16T17:23:33.655863Z",
     "start_time": "2021-03-16T17:23:24.291Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "decode.plot.PlotFrame(sim_frames[torch.randint(0, len(sim_frames), size=(1, ))]).plot()\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(122)\n",
    "decode.plot.PlotFrame(data_frames[torch.randint(0, len(data_frames), size=(1, )), 30:70,-40:]).plot()\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If your available (i.e. free) GPU memory is lower than 6GB you will likely run into memory errors. Check your total GPU memory on the set device by running the cell below.\n",
    "To avoid this you can lower the batch_size to 32 to 16. However you should have at least a 4GB GPU available. You might be able to run training on a smaller GPU using very small batch sizes but this will negatively impact performance.\n",
    "\n",
    "**Note:** If other processes or a screen attached to the GPU you want to train on consumes GPU resources, the free GPU memory might be significantly lower than the total GPU memory size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if device != 'cpu':\n",
    "    mem_gb = torch.cuda.get_device_properties(device).total_memory / 1e9\n",
    "    print(f\"Your approximate total GPU memory size on the set device {device} is {mem_gb:.2f} GB.\")\n",
    "\n",
    "param.HyperParameter.batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are happy with you settings you can write the parameters to a file (or edit the param_friendly.yaml directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-17T12:33:44.772972Z",
     "start_time": "2021-03-17T12:33:44.751297Z"
    }
   },
   "outputs": [],
   "source": [
    "param_out_path = 'notebook_example.yaml' # or an alternative path\n",
    "decode.utils.param_io.save_params(param_out_path, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-05T21:00:50.260177Z",
     "start_time": "2020-10-05T21:00:50.254454Z"
    }
   },
   "source": [
    "# Start the Actual Training of the Model\n",
    "To start training we recommend to start it from a new terminal window/Anaconda prompt as this is most stable. So please open up a new terminal, activate the respective environment and start the training.\n",
    "\n",
    "    conda activate decode_env\n",
    "    cd [directory where this notebook is]\n",
    "    python -m decode.neuralfitter.train.live_engine -p notebook_example.yaml  # change path if you modified it\n",
    "\n",
    "You can end training at any point by killing the associated process if you think that the model has converged. The current network is stored after every epoch.\n",
    "You can also continue training (for example in case of crash) by setting the param.InOut.checkpoint_init parameter to the path of the ckpt.pt of the training run.\n",
    "\n",
    "In case you experience multiprocessing issues, please consult our FAQ in our documentation (https://decode.readthedocs.io/). You may run the command above with the additional `-w 0` flag.\n",
    "\n",
    "To check training progress you may start a tensorboard instance. Again, please open a new terminal instance and navigate (within the terminal, in a new terminal window/tab) to the directory of this notebook and start tensorboard. Don't forget to activate the conda environment before starting tensorboard.\n",
    "\n",
    "    cd [directory which containts 'runs', i.e. this notebook's path]\n",
    "    conda activate decode_env\n",
    "    tensorboard --samples_per_plugin images=100 --port=6006 --logdir=runs\n",
    "\n",
    "Open it in your browser: http://localhost:6006\n",
    "Numereous evaluation metrics are tracked here. Under the 'Images' header you can see example frames and localizations.\n",
    "Keep in mind though that these are all calculated on the simulated data. So you can check whether the training has converged,\n",
    "but you'll have to run the model on the actual data to see if you set the parameters correctly.\n",
    "\n",
    "For that open the Fit.ipynb notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:decode_dev]",
   "language": "python",
   "name": "conda-env-decode_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}